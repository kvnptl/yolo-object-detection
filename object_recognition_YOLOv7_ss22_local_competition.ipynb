{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2yQrsczCsGc"
   },
   "source": [
    "# Yolov7 training and inferencing notebook\n",
    "\n",
    "Reference: [Official YoloV7 github repository](https://github.com/WongKinYiu/yolov7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggh0QkBkC-nl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob as glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnZk1ICNDy-v",
    "outputId": "1fb93a0d-6264-4090-ab51-04851afdbc25"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnz28UfVEIc0"
   },
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxXS00JRI2fQ"
   },
   "source": [
    "Dataset for Summer 2022 competition Source: [HBRS Bib cloud](https://bib-cloud.bib.hochschule-bonn-rhein-sieg.de/apps/files/?dir=/Shared/b-it-bots-ds/atwork/images/object_detection/YOLO/internal_robocup_2022/FULL_DATASET_SS22_COMPETITION&fileid=14231157) (require HBRS library login credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xmzv2t5I4kh"
   },
   "source": [
    "The dataset is structured in the following manner:\n",
    "\n",
    "```\n",
    "├── dataset_ss22_v4_yolov7.yaml\n",
    "├── README.md\n",
    "├── dataset_ss22_v4_yolov7\n",
    "        images\n",
    "        ├── train\n",
    "        └── valid\n",
    "        labels\n",
    "        ├── train\n",
    "        └── valid\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9hbCfxlZ05u"
   },
   "source": [
    "## The Dataset YAML File\n",
    "\n",
    "The dataset YAML (`dataset_ss22_v4_yolov7.yaml`) file containing the path to the training and validation images and labels. This file will also contain the class names from the dataset.\n",
    "\n",
    "The dataset contains 20 classes.\n",
    "\n",
    "The following block shows the contents of the `dataset_ss22_v4_yolov7.yaml` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcdcMP3oZ_cy"
   },
   "source": [
    "```yaml\n",
    "train: ../dataset_ss22_v4_yolov7/images/train \n",
    "val: ../dataset_ss22_v4_yolov7/images/valid\n",
    "\n",
    "nc: 20\n",
    "\n",
    "names: ['F20_20_B', 'R20', 'S40_40_B', 'S40_40_G', 'axis', 'bearing_box', 'bracket', 'brown_box', 'cup', 'dishwasher_soap', 'eye_glasses', 'insulation_tape', 'motor', 'pringles', 'screw_driver', 'sponge', 'spoon', 'tennis_ball', 'toothbrush', 'towel']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4Lo0u6LaBss"
   },
   "source": [
    "## Visualize a Few Ground Truth Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q602BrX1advP"
   },
   "source": [
    "In YOLO format, [x_center, y_center, width, height]\n",
    "\n",
    "\n",
    "```\n",
    "A------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "------------------------B\n",
    "```\n",
    "\n",
    "In Bounding Box format, A [x_min, y_min] and B [x_max, y_max].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize 4 random samples from Dataset [Reference](https://www.youtube.com/watch?v=Ciy1J97dbY0&ab_channel=LearnOpenCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXzvoHfnZ1p_"
   },
   "outputs": [],
   "source": [
    "class_names = ['F20_20_B', 'R20', 'S40_40_B', 'S40_40_G', 'axis', 'bearing_box', 'bracket', 'brown_box', 'cup', \n",
    "               'dishwasher_soap', 'eye_glasses', 'insulation_tape', 'motor', 'pringles', 'screw_driver', 'sponge', \n",
    "               'spoon', 'tennis_ball', 'toothbrush', 'towel']\n",
    "colors = np.random.uniform(0, 255, size=(len(class_names), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.\n",
    "def yolo2bbox(bboxes):\n",
    "    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n",
    "    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n",
    "    return xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqk-XxiPbUSb"
   },
   "outputs": [],
   "source": [
    "def plot_box(image, bboxes, labels):\n",
    "    # Need the image height and width to denormalize\n",
    "    # the bounding box coordinates\n",
    "    h, w, _ = image.shape\n",
    "    for box_num, box in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = yolo2bbox(box)\n",
    "        # denormalize the coordinates\n",
    "        xmin = int(x1*w)\n",
    "        ymin = int(y1*h)\n",
    "        xmax = int(x2*w)\n",
    "        ymax = int(y2*h)\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        class_name = class_names[int(labels[box_num])]\n",
    "\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (xmin, ymin), (xmax, ymax),\n",
    "            color=colors[class_names.index(class_name)],\n",
    "            thickness=2\n",
    "        )\n",
    "\n",
    "        font_scale = min(1, max(3, int(w/500)))\n",
    "        font_thickness = min(2, max(10, int(w/50)))\n",
    "\n",
    "        p1, p2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n",
    "        # Text width and height\n",
    "        tw, th = cv2.getTextSize(\n",
    "            class_name,\n",
    "            0, fontScale=font_scale, thickness=font_thickness\n",
    "        )[0]\n",
    "        p2 = p1[0] + tw, p1[1] + -th - 10\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            p1, p2,\n",
    "            color=colors[class_names.index(class_name)],\n",
    "            thickness=-1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            class_name,\n",
    "            (xmin+1, ymin-10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            font_scale,\n",
    "            (255, 255, 255),\n",
    "            font_thickness\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt_M-eQWb2VC"
   },
   "outputs": [],
   "source": [
    "# Function to plot images with the bounding boxes.\n",
    "def plot(image_paths, label_paths, num_samples):\n",
    "    all_training_images = glob.glob(image_paths)\n",
    "    all_training_labels = glob.glob(label_paths)\n",
    "    all_training_images.sort()\n",
    "    all_training_labels.sort()\n",
    "\n",
    "    num_images = len(all_training_images)\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(num_samples):\n",
    "        j = random.randint(0, num_images-1)\n",
    "        # j = 0\n",
    "        image = cv2.imread(all_training_images[j])\n",
    "        with open(all_training_labels[j], 'r') as f:\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            label_lines = f.readlines()\n",
    "            for label_line in label_lines:\n",
    "                label = label_line.split(' ')[0]\n",
    "                bbox_string = label_line.split(' ')[1:]\n",
    "                x_c, y_c, w, h = bbox_string\n",
    "                x_c = float(x_c)\n",
    "                y_c = float(y_c)\n",
    "                w = float(w)\n",
    "                h = float(h.split('\\n')[0])\n",
    "                bboxes.append([x_c, y_c, w, h])\n",
    "                labels.append(label)\n",
    "        result_image = plot_box(image, bboxes, labels)\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(result_image[:, :, ::-1])\n",
    "        plt.axis('off')\n",
    "    plt.subplots_adjust(wspace=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "-rBlnGadcABY",
    "outputId": "61070f83-7d9f-45f8-9cce-ea726db8f21c"
   },
   "outputs": [],
   "source": [
    "# Visualize a few training images.\n",
    "plot(\n",
    "    image_paths='dataset_ss22_v4_yolov7/images/train/*', \n",
    "    label_paths='dataset_ss22_v4_yolov7/labels/train/*',\n",
    "    num_samples=4,\n",
    ")\n",
    "\n",
    "# plot(\n",
    "#     image_paths='dataset_ss22_inference/train/images/*', \n",
    "#     label_paths='dataset_ss22_inference/train/labels/*',\n",
    "#     num_samples=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDkUcgESddUY"
   },
   "source": [
    "## Clone YOLOV7 Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlIqGP_7dZAp"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('yolov7'):\n",
    "    !git clone https://github.com/WongKinYiu/yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcJVi6Imdf9b",
    "outputId": "2a062772-3414-4d6f-cc00-a4aab3012c29"
   },
   "outputs": [],
   "source": [
    "# Change to yoloV7 directory\n",
    "%cd yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download pretrained weights (if not available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Function to Monitor TensorBoard logs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LSPPYL7dI6u"
   },
   "source": [
    "**NOTE**: TensorBoard logs can be visualized with [Local port link](http://10.20.118.78:31025/#scalars&runSelectionState=eyJ5b2xvNS90cmFpbi9yZXN1bHRzXzEiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzIiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzMiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzQiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzUiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzgiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzgyIjpmYWxzZSwieW9sbzUvdHJhaW4vcmVzdWx0c18xNCI6ZmFsc2UsInlvbG81L3RyYWluL3Jlc3VsdHNfMTMiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzEyIjpmYWxzZSwieW9sbzUvdHJhaW4vcmVzdWx0c18xMSI6ZmFsc2V9) only from HBRS University netowork\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5tNBYgud4df"
   },
   "source": [
    "## Training using YOLOV7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFXnIfvzDz11"
   },
   "outputs": [],
   "source": [
    "# set arguments for training Yolov7\n",
    "\n",
    "TRAIN = True\n",
    "FREEZE = True # freezing first 15 layers\n",
    "MULT_GPU = True\n",
    "GPU_IDs = [0,1] # GPU device ids (default is 0)\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "RESULT_DIR = os.path.expanduser('~') + '/public/logs/yolo7' # set training result directory path\n",
    "WEIGHTS = 'yolov7_training.pt' # pretrained model weights\n",
    "HYP_PARAM = '../config_yolov7/hyp.ss22_local_competition.yaml' # hyperparameter yaml file\n",
    "DATASET_YAML = '../dataset_ss22_v4_yolov7.yaml' # dataset yaml file\n",
    "CFG_YAML = '../config_yolov7/yolov7_ss22.yaml' # set num. of classes and network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc7jxboMczbd"
   },
   "source": [
    "## Helper Functions for Logging\n",
    "\n",
    "The helper functions are for logging of the results in the notebook while training the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7t1wctqcoN-"
   },
   "outputs": [],
   "source": [
    "def set_res_dir():\n",
    "    # Directory to store results\n",
    "    res_dir_count = len(glob.glob(RESULT_DIR + '/train/*'))\n",
    "    print(f\"Current number of result directories: {res_dir_count}\")\n",
    "    if TRAIN:\n",
    "        RES_DIR = f\"{RESULT_DIR}/train/results_{res_dir_count+1}\"\n",
    "        print(RES_DIR)\n",
    "    else:\n",
    "        RES_DIR = f\"{RESULT_DIR}/train/results_{res_dir_count}\"\n",
    "    return RES_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TiGPfG7exH9",
    "outputId": "794e63de-4fa3-430b-c181-89ab7349e92b"
   },
   "outputs": [],
   "source": [
    "if TRAIN:   \n",
    "    if FREEZE:\n",
    "        RES_DIR = set_res_dir()\n",
    "        \n",
    "        if not MULT_GPU:\n",
    "            # trainig on single GPU\n",
    "            !python train.py \\\n",
    "                    --batch-size {BATCH_SIZE} \\\n",
    "                    --nosave \\\n",
    "                    --data {DATASET_YAML} \\\n",
    "                    --cfg {CFG_YAML} \\\n",
    "                    --weights {WEIGHTS} \\\n",
    "                    --hyp {HYP_PARAM} \\\n",
    "                    --epochs {EPOCHS} \\\n",
    "                    --name {RES_DIR} \\\n",
    "                    --device {GPU_IDs[0]} \\\n",
    "                    --img 640 640 \\\n",
    "                    --freeze 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\n",
    "\n",
    "        else:\n",
    "\n",
    "            # trainig on multi GPUs\n",
    "            !python -m torch.distributed.launch --nproc_per_node {len(GPU_IDs)} train.py \\\n",
    "                    --batch-size {BATCH_SIZE} \\\n",
    "                    --sync-bn \\\n",
    "                    --nosave \\\n",
    "                    --data {DATASET_YAML} \\\n",
    "                    --cfg {CFG_YAML} \\\n",
    "                    --weights {WEIGHTS} \\\n",
    "                    --hyp {HYP_PARAM} \\\n",
    "                    --epochs {EPOCHS} \\\n",
    "                    --name {RES_DIR} \\\n",
    "                    --device {str(GPU_IDs).replace('[','').replace(']','').replace(' ', '')} \\\n",
    "                    --img 640 640 \\\n",
    "                    --freeze 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\n",
    "    \n",
    "    else:\n",
    "        RES_DIR = set_res_dir()\n",
    "        # training all layers of model\n",
    "        if not MULT_GPU:\n",
    "                    # trainig on single GPU\n",
    "                    !python train.py \\\n",
    "                            --batch-size {BATCH_SIZE} \\\n",
    "                            --nosave \\\n",
    "                            --data {DATASET_YAML} \\\n",
    "                            --cfg {CFG_YAML} \\\n",
    "                            --weights {WEIGHTS} \\\n",
    "                            --hyp {HYP_PARAM} \\\n",
    "                            --epochs {EPOCHS} \\\n",
    "                            --name {RES_DIR} \\\n",
    "                            --device {GPU_IDs[0]} \\\n",
    "                            --img 640 640 \\\n",
    "\n",
    "        else:\n",
    "            # trainig on multi GPUs\n",
    "            !python -m torch.distributed.launch --nproc_per_node {len(GPU_IDs)} train.py \\\n",
    "                    --batch-size {BATCH_SIZE} \\\n",
    "                    --sync-bn \\\n",
    "                    --nosave \\\n",
    "                    --data {DATASET_YAML} \\\n",
    "                    --cfg {CFG_YAML} \\\n",
    "                    --weights {WEIGHTS} \\\n",
    "                    --hyp {HYP_PARAM} \\\n",
    "                    --epochs {EPOCHS} \\\n",
    "                    --name {RES_DIR} \\\n",
    "                    --device {str(GPU_IDs).replace('[','').replace(']','').replace(' ', '')} \\\n",
    "                    --img 640 640 \\\n",
    "\n",
    "\n",
    "else:\n",
    "    # set the RES_DIR number\n",
    "    res_dir_count = '1' \n",
    "    RES_DIR = f\"{RESULT_DIR}/train/results_{res_dir_count}\"\n",
    "    print(\"Set RES_DIR to: \", RES_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwfG5GQwqYoM"
   },
   "source": [
    "## Check Out the Validation Predictions and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyrImEH2qcN1"
   },
   "source": [
    "### Visualization and Inference Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEKKeFxpqfMs"
   },
   "outputs": [],
   "source": [
    "# Function to show validation predictions saved during training.\n",
    "def show_valid_results(RES_DIR):\n",
    "    !ls {RES_DIR}\n",
    "    EXP_PATH = f\"{RES_DIR}\"\n",
    "    validation_pred_images = glob.glob(f\"{EXP_PATH}/*_pred.jpg\") # TODO: detect all image ext files\n",
    "    print(validation_pred_images)\n",
    "    for pred_image in validation_pred_images:\n",
    "        image = cv2.imread(pred_image)\n",
    "        plt.figure(figsize=(19, 16))\n",
    "        plt.imshow(image[:, :, ::-1])\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAF0wDI3qhJJ"
   },
   "source": [
    "The following functions are for carrying out inference on images and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy4VOSz0qifJ"
   },
   "outputs": [],
   "source": [
    "# Helper function for inference on images.\n",
    "def inference(RES_DIR, data_path):\n",
    "    # Directory to store inference results.\n",
    "    infer_dir_count = len(glob.glob(RESULT_DIR + '/detect/*'))\n",
    "    print(f\"Current number of inference detection directories: {infer_dir_count}\")\n",
    "    INFER_DIR = f\"{RESULT_DIR}/detect/inference_{infer_dir_count+1}\"\n",
    "    print(INFER_DIR)\n",
    "    # Inference on images.\n",
    "    !python detect.py \\\n",
    "    --weights {RES_DIR}/weights/best.pt \\\n",
    "    --source {data_path} \\\n",
    "    --name {INFER_DIR} \\\n",
    "    --device 0 \\\n",
    "    --conf 0.60 \\\n",
    "    --img-size 640\n",
    "    \n",
    "    return INFER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-j53ehiqmZU"
   },
   "outputs": [],
   "source": [
    "def visualize(INFER_DIR):\n",
    "# Visualize inference images.\n",
    "    INFER_PATH = f\"{INFER_DIR}\"\n",
    "    infer_images = glob.glob(f\"{INFER_PATH}/*\")\n",
    "    print(infer_images)\n",
    "    for pred_image in infer_images:\n",
    "        image = cv2.imread(pred_image)\n",
    "        plt.figure(figsize=(19, 16))\n",
    "        plt.imshow(image[:, :, ::-1])\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtKTNNDoqqi3"
   },
   "source": [
    "**Visualize validation prediction images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p_punILPqqWp",
    "outputId": "aeb5a1c2-f00c-4eee-f3ae-e7e82133aaba"
   },
   "outputs": [],
   "source": [
    "show_valid_results(RES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jurecthtqvj9"
   },
   "source": [
    "## Inference\n",
    "In this section, we will carry out inference on unseen images and videos from the internet. \n",
    "\n",
    "The images for inference are in the `inference_images` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTigK5v4q27c"
   },
   "source": [
    "**To carry out inference on images, we just need to provide the directory path where all the images are stored, and inference will happen on all images automatically.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0k-GZGFq4IS",
    "outputId": "5456023a-da3f-460e-bf0a-6c396d4d2928"
   },
   "outputs": [],
   "source": [
    "on_single_image = True\n",
    "\n",
    "if on_single_image:\n",
    "    # Inference on single image\n",
    "    IMAGE_INFER_DIR = inference(RES_DIR, '../inference_images/inference_img01/1562121558.622500193_raw_rgb.jpg')\n",
    "else:\n",
    "    # Inference on images.\n",
    "    IMAGE_INFER_DIR = inference(RES_DIR, '../day3_test_images')\n",
    "\n",
    "\n",
    "IMAGE_INFER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JLDCTeBfq5jO",
    "outputId": "13fa46d6-fbaa-401e-e7cc-14c82b4e3b6b"
   },
   "outputs": [],
   "source": [
    "# IMAGE_INFER_DIR\n",
    "visualize(IMAGE_INFER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model (.pt) to ONNX model (.onnx)\n",
    "\n",
    "Note: The exported ONNX model is not yet tested in C++ (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python export.py \\\n",
    "    --weights {RESULT_DIR}/train/results_1/weights/best.pt \\\n",
    "    --grid \\\n",
    "    --end2end \\\n",
    "    --simplify \\\n",
    "    --topk-all 100 \\\n",
    "    --iou-thres 0.65 \\\n",
    "    --conf-thres 0.35 \\\n",
    "    --img-size 640 640 \\\n",
    "    --max-wh 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "custom_object_detection_using_YOLOv5_opencv.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
