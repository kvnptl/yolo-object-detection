{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "m2yQrsczCsGc"
   },
   "source": [
    "Reference: [Ultralytics YOLOv8 github repository](https://github.com/ultralytics/ultralytics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YDkUcgESddUY"
   },
   "source": [
    "## YOLOv8 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlIqGP_7dZAp"
   },
   "outputs": [],
   "source": [
    "# install ultralytics yolov8\n",
    "!pip install ultralytics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and Hyperparameters for training\n",
    "data_yaml = f\"/home/kpatel2s/kpatel2s/b_it_bots/2d_object_detection/yolo-object-detection/dataset_ss22_v4.yaml\"\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "image_size = 640\n",
    "model_name = \"yolov8_ss22_v4\"\n",
    "cuda_devices = [0,1] # GPU devices ids \n",
    "freeze_layers = '0, 1, 2, 3, 4, 5' # Layer ids to freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # !python -m torch.distributed.run --nproc_per_node 2 train.py \\\n",
    "        #         --batch 512 \\ DONE\n",
    "        #         --data ../dataset_ss22_v4.yaml \\ DONE\n",
    "        #         --weights yolov5m.pt \\ \n",
    "        #         --img 640 \\ DONE\n",
    "        #         --epochs {EPOCHS} \\ DONE\n",
    "        #         --name {RES_DIR} \\ DONE\n",
    "        #         --device 0,1 \\ DONE\n",
    "        #         --freeze 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcJVi6Imdf9b",
    "outputId": "2a062772-3414-4d6f-cc00-a4aab3012c29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.112 ðŸš€ Python-3.8.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "                                                            CUDA:1 (NVIDIA GeForce RTX 3090, 24265MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/home/kpatel2s/kpatel2s/b_it_bots/2d_object_detection/yolo-object-detection/dataset_ss22_v4.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=[0, 1], workers=8, project=None, name=yolov8_ss22_v4, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/yolov8_ss22_v44\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "DDP command: ['/home/kpatel2s/miniconda3/envs/yolo_training/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node', '2', '--master_port', '44831', '/home/kpatel2s/.local/lib/python3.8/site-packages/ipykernel_launcher.py']\n",
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\n",
      "\n",
      "To exit, you will have to explicitly quit this process, by either sending\n",
      "\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\n",
      "\n",
      "To read more about this, see https://github.com/ipython/ipython/issues/2049\n",
      "\n",
      "\n",
      "To connect another client to this kernel, use:\n",
      "    --existing kernel-925163.json\n",
      "NOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\n",
      "\n",
      "To exit, you will have to explicitly quit this process, by either sending\n",
      "\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\n",
      "\n",
      "To read more about this, see https://github.com/ipython/ipython/issues/2049\n",
      "\n",
      "\n",
      "To connect another client to this kernel, use:\n",
      "    --existing kernel-925162.json\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO('yolov8n.yaml')  # build a new model from YAML\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "# model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n",
    "\n",
    "# Train the model\n",
    "model.train(data=data_yaml,  # dataset.yaml \n",
    "            epochs=epochs,\n",
    "            imgsz=image_size,\n",
    "            batch=batch_size,\n",
    "            name=model_name,\n",
    "            device=cuda_devices,\n",
    "            )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Monitor TensorBoard logs**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2LSPPYL7dI6u"
   },
   "source": [
    "**NOTE**: TensorBoard logs can be visualized with [Local port link](http://10.20.118.78:31025/#scalars&runSelectionState=eyJ5b2xvNS90cmFpbi9yZXN1bHRzXzEiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzIiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzMiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzQiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzUiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzgiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzgyIjpmYWxzZSwieW9sbzUvdHJhaW4vcmVzdWx0c18xNCI6ZmFsc2UsInlvbG81L3RyYWluL3Jlc3VsdHNfMTMiOmZhbHNlLCJ5b2xvNS90cmFpbi9yZXN1bHRzXzEyIjpmYWxzZSwieW9sbzUvdHJhaW4vcmVzdWx0c18xMSI6ZmFsc2V9)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A5tNBYgud4df"
   },
   "source": [
    "## Training using YOLOV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFXnIfvzDz11"
   },
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "FREEZE = True # freezing first 15 layers\n",
    "EPOCHS = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If already trained model (.pt file) is available, then give that model path in `--weights` argument\n",
    "Note: User configurable arguments\n",
    "- batch size\n",
    "- data yaml path\n",
    "- pre-trained weight file\n",
    "- image size\n",
    "- epochs\n",
    "- result directory\n",
    "- freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layer(trainer):\n",
    "    model = trainer.model\n",
    "    num_freeze = 10\n",
    "    print(f\"Freezing {num_freeze} layers\")\n",
    "    freeze = [f'model.{x}.' for x in range(num_freeze)]  # layers to freeze \n",
    "    for k, v in model.named_parameters(): \n",
    "        v.requires_grad = True  # train all layers \n",
    "        if any(x in k for x in freeze): \n",
    "            print(f'freezing {k}') \n",
    "            v.requires_grad = False \n",
    "    print(f\"{num_freeze} layers are freezed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.add_callback(\"on_train_start\", freeze_layer)\n",
    "model.train(data=\"/home/tapendra/Desktop/config/data.yaml\", epochs=2, batch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TiGPfG7exH9",
    "outputId": "794e63de-4fa3-430b-c181-89ab7349e92b"
   },
   "outputs": [],
   "source": [
    "\n",
    "if TRAIN:   \n",
    "    if FREEZE:\n",
    "        RES_DIR = set_res_dir()\n",
    "    \n",
    "        # training by freezing first 15 layers out of 25 layers       \n",
    "#         !python train.py \\\n",
    "#                 --batch 256 \\\n",
    "#                 --data ../dataset_ss22.yaml \\\n",
    "#                 --weights yolov5m.pt \\\n",
    "#                 --img 640 \\\n",
    "#                 --epochs {EPOCHS} \\\n",
    "#                 --name {RES_DIR} \\\n",
    "#                 --freeze 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\n",
    "        \n",
    "        # trainig on multi GPUs\n",
    "        !python -m torch.distributed.run --nproc_per_node 2 /home/kpatel2s/kpatel2s/b_it_bots/2d_object_detection/yolo-object-detection/yolov5/train.py \\\n",
    "                --batch 512 \\\n",
    "                --data ../dataset_ss22_v4.yaml \\\n",
    "                --weights yolov5m.pt \\\n",
    "                --img 640 \\\n",
    "                --epochs {EPOCHS} \\\n",
    "                --name {RES_DIR} \\\n",
    "                --device 0,1 \\\n",
    "                --freeze 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\n",
    "        \n",
    "        \n",
    "    \n",
    "#         # trainig using pretrained model (multi GPUs)\n",
    "#         !python -m torch.distributed.run --nproc_per_node 2 train.py \\\n",
    "#                 --batch 512 \\\n",
    "#                 --data ../dataset_ss22_v3.yaml \\\n",
    "#                 --resume /home/jovyan/public/logs/yolo5/train/results_22/weights/best.pt \\\n",
    "#                 --img 640 \\\n",
    "#                 --epochs {EPOCHS} \\\n",
    "#                 --name {RES_DIR} \\\n",
    "#                 --device 0,1 \\\n",
    "#                 --freeze 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\n",
    "    \n",
    "    else:\n",
    "        RES_DIR = set_res_dir()\n",
    "        # training all layers of model\n",
    "        !python train.py --data ../dataset_ss22_v2.yaml --weights yolov5m.pt \\\n",
    "        --img 640 --epochs {EPOCHS} --batch-size 256 --name {RES_DIR}\n",
    "else:\n",
    "    # set the RES_DIR name\n",
    "    res_dir_count = '1' \n",
    "    RES_DIR = f\"/home/kpatel2s/kpatel2s/b_it_bots/2d_object_detection/yolo-object-detection/logs/yolov5/train/results_{res_dir_count}\"\n",
    "    print(\"Set RES_DIR to: \", RES_DIR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wwfG5GQwqYoM"
   },
   "source": [
    "## Check Out the Validation Predictions and Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SyrImEH2qcN1"
   },
   "source": [
    "### Visualization and Inference Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEKKeFxpqfMs"
   },
   "outputs": [],
   "source": [
    "# Function to show validation predictions saved during training.\n",
    "def show_valid_results(RES_DIR):\n",
    "    !ls {RES_DIR}\n",
    "    EXP_PATH = f\"{RES_DIR}\"\n",
    "    validation_pred_images = glob.glob(f\"{EXP_PATH}/*_pred.jpg\")\n",
    "    print(validation_pred_images)\n",
    "    for pred_image in validation_pred_images:\n",
    "        image = cv2.imread(pred_image)\n",
    "        plt.figure(figsize=(19, 16))\n",
    "        plt.imshow(image[:, :, ::-1])\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QAF0wDI3qhJJ"
   },
   "source": [
    "The following functions are for carrying out inference on images and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy4VOSz0qifJ"
   },
   "outputs": [],
   "source": [
    "# Helper function for inference on images.\n",
    "def inference(RES_DIR, data_path):\n",
    "    # Directory to store inference results.\n",
    "    infer_dir_count = len(glob.glob('/home/kpatel2s/kpatel2s/b_it_bots/2d_object_detection/yolo-object-detection/logs/yolov5/detect/*'))\n",
    "    print(f\"Current number of inference detection directories: {infer_dir_count}\")\n",
    "    INFER_DIR = f\"/home/kpatel2s/kpatel2s/b_it_bots/2d_object_detection/yolo-object-detection/logs/yolov5/detect/inference_{infer_dir_count+1}\"\n",
    "    print(INFER_DIR)\n",
    "    # Inference on images.\n",
    "    !python detect.py --weights {RES_DIR}/weights/best.pt \\\n",
    "    --source {data_path} --name {INFER_DIR} --device 0\n",
    "    return INFER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-j53ehiqmZU"
   },
   "outputs": [],
   "source": [
    "def visualize(INFER_DIR):\n",
    "# Visualize inference images.\n",
    "    INFER_PATH = f\"{INFER_DIR}\"\n",
    "    infer_images = glob.glob(f\"{INFER_PATH}/*\")\n",
    "    print(infer_images)\n",
    "    for pred_image in infer_images:\n",
    "        image = cv2.imread(pred_image)\n",
    "        plt.figure(figsize=(19, 16))\n",
    "        plt.imshow(image[:, :, ::-1])\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UtKTNNDoqqi3"
   },
   "source": [
    "**Visualize validation prediction images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p_punILPqqWp",
    "outputId": "aeb5a1c2-f00c-4eee-f3ae-e7e82133aaba"
   },
   "outputs": [],
   "source": [
    "show_valid_results(RES_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jurecthtqvj9"
   },
   "source": [
    "### Inference\n",
    "In this section, we will carry out inference on unseen images and videos from the internet. \n",
    "\n",
    "The images for inference are in the `inference_images` directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RTigK5v4q27c"
   },
   "source": [
    "**To carry out inference on images, we just need to provide the directory path where all the images are stored, and inference will happen on all images automatically.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0k-GZGFq4IS",
    "outputId": "5456023a-da3f-460e-bf0a-6c396d4d2928"
   },
   "outputs": [],
   "source": [
    "on_single_image = True\n",
    "\n",
    "if on_single_image:\n",
    "    # Inference on single image\n",
    "    IMAGE_INFER_DIR = inference(RES_DIR, '/home/kpatel2s/kpatel2s/b_it_bots/2d_object_detection/yolo-object-detection/test_imgs/rgb_raw_1662123101.237838.jpg')\n",
    "else:\n",
    "    # Inference on images.\n",
    "    IMAGE_INFER_DIR = inference(RES_DIR, '/home/kpatel2s/kpatel2s/b_it_bots/2d_object_detection/yolo-object-detection/test_imgs')\n",
    "\n",
    "\n",
    "IMAGE_INFER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JLDCTeBfq5jO",
    "outputId": "13fa46d6-fbaa-401e-e7cc-14c82b4e3b6b"
   },
   "outputs": [],
   "source": [
    "# IMAGE_INFER_DIR\n",
    "visualize(IMAGE_INFER_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export model (.pt) to ONNX model (.onnx)\n",
    "###### Reference: https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python export.py --weights /home/jovyan/public/logs/yolo5/train/results_28/weights/best.pt --include onnx"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "custom_object_detection_using_YOLOv5_opencv.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
